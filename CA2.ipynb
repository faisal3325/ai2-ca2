{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def time_elapsed_start():\n",
    "    return time.time()\n",
    "\n",
    "def time_elapsed_stop(start):\n",
    "    print(f\"Execution took {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "ROWS_TO_READ = 200000\n",
    "df = pd.read_csv('quora_questions.csv', nrows=ROWS_TO_READ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 question\n162665  What are some major events that happened in 1994?\n184379  I am a fourth year student of BA LLB from Indi...\n22851                      How do I recover from failure?\n150260  Objectively speaking, what have been some of P...\n77232   How did the Commercial Revolution impact Europ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>162665</td>\n      <td>What are some major events that happened in 1994?</td>\n    </tr>\n    <tr>\n      <td>184379</td>\n      <td>I am a fourth year student of BA LLB from Indi...</td>\n    </tr>\n    <tr>\n      <td>22851</td>\n      <td>How do I recover from failure?</td>\n    </tr>\n    <tr>\n      <td>150260</td>\n      <td>Objectively speaking, what have been some of P...</td>\n    </tr>\n    <tr>\n      <td>77232</td>\n      <td>How did the Commercial Revolution impact Europ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataframe size: 200000\n"
    }
   ],
   "source": [
    "# dataframe size\n",
    "print(f\"Dataframe size: {df.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Execution took 3.0968377590179443 seconds\n"
    }
   ],
   "source": [
    "start = time_elapsed_start()\n",
    "term_matrix = count_vectorizer.fit_transform(df['question'])\n",
    "time_elapsed_stop(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<200000x27884 sparse matrix of type '<class 'numpy.int64'>'\n\twith 981746 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector has taken 27,884 words in the vocabulary from 2,00,000 questions(rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=13, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n                          evaluate_every=-1, learning_decay=0.7,\n                          learning_method='batch', learning_offset=10.0,\n                          max_doc_update_iter=100, max_iter=10,\n                          mean_change_tol=0.001, n_components=13, n_jobs=None,\n                          perp_tol=0.1, random_state=1, topic_word_prior=None,\n                          total_samples=1000000.0, verbose=0)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "start = time_elapsed_start()\n",
    "lda.fit(term_matrix)\n",
    "time_elapsed_stop(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking log likelihood, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time_elapsed_start()\n",
    "log_likelihood = lda.score(term_matrix)\n",
    "perplexity = lda.perplexity(term_matrix)\n",
    "time_elapsed_stop(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Log likelihood: -8516737.535031516\nPerplexity: 4612.6439753800405\n"
    }
   ],
   "source": [
    "print(f\"Log likelihood: {log_likelihood}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a metrics dataframe for comparison later\n",
    "lda_metrics_df = pd.DataFrame([[log_likelihood, perplexity, 13, 0.7]], columns=['log_likelihood', 'perplexity', 'n_components', 'learning_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   log_likelihood  perplexity\n0   -8.507502e+06   4570.6395",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>log_likelihood</th>\n      <th>perplexity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>-8.507502e+06</td>\n      <td>4570.6395</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "lda_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying GridSearch for finding out best parameters for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing GridSearchCV from sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating parameters for LDA\n",
    "params = {'n_components': [5, 7, 9, 10, 12, 14], 'learning_decay': [.3, .5, .7, .9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing evaluate LDA and passing parameters to it\n",
    "eval_lda = LatentDirichletAllocation()\n",
    "eval_lda = GridSearchCV(eval_lda, param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "# this will search for optimal parameters\n",
    "# Note: this process consumes a significant amount of time and resources\n",
    "start = time_elapsed_start()\n",
    "eval_lda.fit(term_matrix)\n",
    "time_elapsed_stop(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model which gave highest score \n",
    "best_lda_model = lda_comparison.best_estimator_\n",
    "\n",
    "# Model Parameters is used to store a list of parameter settings dicts for all the parameter candidates\n",
    "print(\"Best Model's Params: \", lda_comparison.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", lda_comparison.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n                          evaluate_every=-1, learning_decay=0.7,\n                          learning_method='batch', learning_offset=10.0,\n                          max_doc_update_iter=100, max_iter=10,\n                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n                          perp_tol=0.1, random_state=3, topic_word_prior=None,\n                          total_samples=1000000.0, verbose=0)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "lda.fit(term_matrix)print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time_elapsed_start()\n",
    "log_likelihood = lda.score(term_matrix)\n",
    "perplexity = lda.perplexity(term_matrix)\n",
    "time_elapsed_stop(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Log likelihood: -8507502.497878691\nPerplexity: 4570.639499601198\n"
    }
   ],
   "source": [
    "print(f\"Log likelihood: {log_likelihood}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing metrics in metrics dataframe\n",
    "lda_metrics_df.loc[-1] = [log_likelihood, perplexity, 10, 0.7]\n",
    "lda_metrics_df.index = lda_metrics_df.index + 1\n",
    "lda_metrics_df = lda_metrics_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   log_likelihood  perplexity\n0   -8.507502e+06   4570.6395\n1             NaN         NaN\n2   -8.507502e+06   4570.6395",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>log_likelihood</th>\n      <th>perplexity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>-8.507502e+06</td>\n      <td>4570.6395</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>-8.507502e+06</td>\n      <td>4570.6395</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "lda_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring topics and their words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 27,885 words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 topics with 27,885 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}